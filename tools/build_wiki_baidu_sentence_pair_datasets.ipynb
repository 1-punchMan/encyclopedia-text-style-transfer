{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adaptive-timber",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zchen/miniconda3/envs/text_style_transfer/lib/python3.9/site-packages/requests/__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.8) or chardet (5.0.0)/charset_normalizer (2.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import json, os, re, random\n",
    "from ckiptagger import data_utils, construct_dictionary, WS, POS, NER\n",
    "from opencc import OpenCC\n",
    "cc_t2s = OpenCC('t2s')\n",
    "cc_s2t = OpenCC('s2t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "subject-cursor",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-31 04:00:56.940540: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22307 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:17:00.0, compute capability: 8.6\n",
      "2022-10-31 04:01:00.382183: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22307 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:17:00.0, compute capability: 8.6\n",
      "2022-10-31 04:01:03.860793: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22307 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:17:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "path = \"/home/zchen/encyclopedia-text-style-transfer/tools/ckip/\"\n",
    "data_utils.download_data_url(path)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "path = path + \"data\"\n",
    "ws = WS(path, disable_cuda=False)\n",
    "pos = POS(path, disable_cuda=False)\n",
    "ner = NER(path, disable_cuda=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "monetary-interval",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(file):\n",
    "    with open(file, 'r', encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def save_json(file, obj):\n",
    "    with open(file, 'w', encoding=\"utf-8\") as f:\n",
    "        json.dump(obj, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fresh-practice",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"/home/zchen/encyclopedia-text-style-transfer/data/ETST/wiki_baidu_articles_pairs.json\"\n",
    "art_pairs = load_json(file)\n",
    "print(len(art_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "quarterly-capital",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115 98\n",
      "98 115\n",
      "(1 / 44)\n",
      "177 352\n",
      "352 177\n",
      "(2 / 44)\n",
      "62 5\n",
      "5 62\n",
      "(3 / 44)\n",
      "22 11\n",
      "11 22\n",
      "(4 / 44)\n",
      "110 104\n",
      "104 110\n",
      "(5 / 44)\n",
      "13 83\n",
      "83 13\n",
      "(6 / 44)\n",
      "13 104\n",
      "104 13\n",
      "(7 / 44)\n",
      "29 70\n",
      "70 29\n",
      "(8 / 44)\n",
      "131 174\n",
      "174 131\n",
      "(9 / 44)\n",
      "22 33\n",
      "33 22\n",
      "(10 / 44)\n",
      "79 154\n",
      "154 79\n",
      "(11 / 44)\n",
      "62 25\n",
      "25 62\n",
      "(12 / 44)\n",
      "4 10\n",
      "10 4\n",
      "(13 / 44)\n",
      "28 105\n",
      "105 28\n",
      "(14 / 44)\n",
      "43 19\n",
      "19 43\n",
      "(15 / 44)\n",
      "18 114\n",
      "114 18\n",
      "(16 / 44)\n",
      "67 101\n",
      "101 67\n",
      "(17 / 44)\n",
      "15 5\n",
      "5 15\n",
      "(18 / 44)\n",
      "224 205\n",
      "205 224\n",
      "(19 / 44)\n",
      "13 3\n",
      "3 13\n",
      "(20 / 44)\n",
      "20 39\n",
      "39 20\n",
      "(21 / 44)\n",
      "4 32\n",
      "32 4\n",
      "(22 / 44)\n",
      "78 40\n",
      "40 78\n",
      "(23 / 44)\n",
      "32 28\n",
      "28 32\n",
      "(24 / 44)\n",
      "10 61\n",
      "61 10\n",
      "(25 / 44)\n",
      "91 2\n",
      "2 91\n",
      "(26 / 44)\n",
      "33 55\n",
      "55 33\n",
      "(27 / 44)\n",
      "19 137\n",
      "137 19\n",
      "(28 / 44)\n",
      "98 111\n",
      "111 98\n",
      "(29 / 44)\n",
      "137 146\n",
      "146 137\n",
      "(30 / 44)\n",
      "36 73\n",
      "73 36\n",
      "(31 / 44)\n",
      "19 35\n",
      "35 19\n",
      "(32 / 44)\n",
      "17 140\n",
      "140 17\n",
      "(33 / 44)\n",
      "76 401\n",
      "401 76\n",
      "(34 / 44)\n",
      "18 47\n",
      "47 18\n",
      "(35 / 44)\n",
      "102 13\n",
      "13 102\n",
      "(36 / 44)\n",
      "17 24\n",
      "24 17\n",
      "(37 / 44)\n",
      "51 43\n",
      "43 51\n",
      "(38 / 44)\n",
      "37 66\n",
      "66 37\n",
      "(39 / 44)\n",
      "68 113\n",
      "113 68\n",
      "(40 / 44)\n",
      "14 2\n",
      "2 14\n",
      "(41 / 44)\n",
      "147 79\n",
      "79 147\n",
      "(42 / 44)\n",
      "6 10\n",
      "10 6\n",
      "(43 / 44)\n",
      "2 29\n",
      "29 2\n",
      "(44 / 44)\n"
     ]
    }
   ],
   "source": [
    "def get_baidu_sents_str_from_pairs(pairs):\n",
    "    sents = []\n",
    "    for src, targs in pairs:\n",
    "        sents += [targ for targ, score in targs]\n",
    "    return '\\n'.join(sents)\n",
    "\n",
    "def arts2sents_str(arts):\n",
    "    return '\\n'.join(list(arts.values()))\n",
    "    \n",
    "def get_s_t(s):\n",
    "    s_s, s_t = cc_t2s.convert(s), cc_s2t.convert(s)\n",
    "    s_s = [sent for sent in s_s.split('\\n') if len(sent) > 5]\n",
    "    s_t = [sent for sent in s_t.split('\\n') if len(sent) > 5]\n",
    "    assert len(s_s) == len(s_t), (len(s_s), len(s_t))\n",
    "    \n",
    "    zipped = list(zip(s_s, s_t))\n",
    "    zipped = list(set(zipped))\n",
    "    s_s, s_t = list(zip(*zipped))\n",
    "    assert len(s_s) == len(s_t), (len(s_s), len(s_t))\n",
    "    return s_s, s_t\n",
    "    \n",
    "def sents2ents(sentence_list):\n",
    "    word_sentence_list = ws(\n",
    "        sentence_list,\n",
    "        # sentence_segmentation = True, # To consider delimiters\n",
    "        # segment_delimiter_set = {\",\", \"。\", \":\", \"?\", \"!\", \";\"}), # This is the defualt set of delimiters\n",
    "        # recommend_dictionary = dictionary1, # words in this dictionary are encouraged\n",
    "        # coerce_dictionary = dictionary2, # words in this dictionary are forced\n",
    "    )\n",
    "    pos_sentence_list = pos(word_sentence_list)\n",
    "    entity_sentence_list = ner(word_sentence_list, pos_sentence_list)\n",
    "    \n",
    "    ent_sents = []\n",
    "    for sent, word_sent, pos_sent, ent_sent in zip(sentence_list, word_sentence_list, pos_sentence_list, entity_sentence_list):\n",
    "        ents = [word for word, pos in zip(word_sent, pos_sent) if re.match(r\"N[a-d]\", pos)]\n",
    "        # re.match() determines whether the pattern matches the beginning of the string,\n",
    "        # returns the matching object if it matches, and returns None if it does not match.\n",
    "        \n",
    "        ents += [ent[3] for ent in ent_sent]\n",
    "        ent_sents.append(set(ents))\n",
    "    return ent_sents\n",
    "    \n",
    "def get_score(src_ent_sent, targ_ent_sent):\n",
    "    intersection_len = len(src_ent_sent & targ_ent_sent)\n",
    "    if intersection_len == 0:\n",
    "        return 0\n",
    "    precision = intersection_len / len(targ_ent_sent)\n",
    "    recall = intersection_len / len(src_ent_sent)\n",
    "    targ_len = len(targ_ent_sent)\n",
    "    return (2*precision*recall) / (precision+recall)\n",
    "    \n",
    "def filter_sent(zipped, p):\n",
    "    \"\"\" filtering strategy, e.g., highest k sents or a threshold score p \"\"\"\n",
    "#     return zipped[:k]\n",
    "    \n",
    "    sents = []\n",
    "    for sent, score in zipped:\n",
    "        if score >= p:\n",
    "            sents.append((sent, score))\n",
    "        else:\n",
    "            break\n",
    "    return sents\n",
    "\n",
    "def get_pair(src_sent, targ_sents, src_ent_sent, targ_ent_sents):\n",
    "    scores = [get_score(src_ent_sent, targ_ent_sent) for targ_ent_sent in targ_ent_sents]\n",
    "    zipped = list(zip(targ_sents, scores))\n",
    "    zipped.sort(key=lambda x: x[1], reverse=True)\n",
    "    matched = filter_sent(zipped, p=0.3)\n",
    "    return (src_sent, matched)\n",
    "\n",
    "def get_pairs(src_sents_str, targ_sents_str):\n",
    "    src_sents_s, src_sents_t = get_s_t(src_sents_str)\n",
    "    targ_sents_s, targ_sents_t = get_s_t(targ_sents_str)\n",
    "    print(len(src_sents_s), len(targ_sents_s))\n",
    "    src_ent_sents, targ_ent_sents = sents2ents(src_sents_t), sents2ents(targ_sents_t)\n",
    "    return [get_pair(src_sent, targ_sents_s, src_ent_sent, targ_ent_sents) for src_sent, src_ent_sent in zip(src_sents_s, src_ent_sents)]\n",
    "\n",
    "sent_pairs_w2b, sent_pairs_b2w = [], []\n",
    "\"\"\"\n",
    "[\n",
    "    (src_sent_1, [(targ_sent_n, score), ...]),\n",
    "    ...\n",
    "    ]\n",
    "\"\"\"\n",
    "\n",
    "total = len(art_pairs)\n",
    "for i, (title, (wiki_arts, baidu_arts)) in enumerate(art_pairs.items()):\n",
    "    wiki_sents_str, baidu_sents_str = arts2sents_str(wiki_arts), arts2sents_str(baidu_arts)\n",
    "    wiki2baidu = get_pairs(wiki_sents_str, baidu_sents_str)    # src: wiki, targ: baidu\n",
    "    baidu2wiki = get_pairs(baidu_sents_str, wiki_sents_str)    # src: baidu, targ: wiki\n",
    "    sent_pairs_w2b += wiki2baidu\n",
    "    sent_pairs_b2w += baidu2wiki\n",
    "    print(f\"({i + 1} / {total})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "first-climb",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"/home/zchen/encyclopedia-text-style-transfer/data/ETST/sents_pairs_b2w.json\"\n",
    "save_json(file, sent_pairs_b2w)\n",
    "file = \"/home/zchen/encyclopedia-text-style-transfer/data/ETST/sents_pairs_w2b.json\"\n",
    "save_json(file, sent_pairs_w2b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "pressed-hopkins",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4758 2.0\n",
      "[('金门县市区公车由金门县县营事业机构金门县公共车船管理处营运 ，有四个营运车站：金城车站、山外车站、沙美车站、烈屿车站。总计有29线编有编号的一般公车路线，以及5条台湾好行观光公车路线。', []), ('金门县辖有三镇（金城镇、金湖镇、金沙镇）、三乡（金宁乡、烈屿乡、乌坵乡（代管））。', [('金门县辖3个镇、2个乡，分别是金城镇、金湖镇、金沙镇、金宁乡、烈屿乡。', 0.6666666666666666), ('1954年6月，台当局在其控制的莆田县乌丘屿设置乌丘乡（乌丘村），由于莆田县主体已全部由中国大陆统治，故指定暂由金门县代管，此时金门县辖金城镇、金沙镇、金宁乡、金湖乡、金山乡、烈屿乡、乌丘乡（乌丘村）等二镇五乡。', 0.3888888888888889)]), ('另有代管的乌坵乡二岛屿（大坵与小坵），则地处东经119度28分，北纬24度59分，位在中华人民共和国福建省莆田外海。离金门本岛相距72海浬，大约位于金门与马祖中心点。', []), ('金门县县长是金门县政府之行政首长，负责综理县政，并指挥、监督所属职员及机构。现任金门县长杨镇浯为中国国民党籍第7届金门县县长。', []), ('民间文学方面的著作及编辑，有：林永塘《浯洲俗谚集》、吴家箴《浯岛情怀》、许丕华《浯乡俗谚风华录》、唐蕙韵整理《金门民间文学集.传说故事卷》。', []), ('2013年起，厦门大嶝岛与小嶝岛间填海造陆，越界盗采海砂，导致金门沿岸国土流失，古迹、碉堡设施损毁，海岸线侵蚀倒退情况严重。', []), ('几十年来，金门一直面临着居民供水困难的问题，原因包括湖水浅、雨量少、地理条件限制等，使得建水库和水坝难以实现。因此，金门经常过度使用地下水，导致潮洪上升，土壤盐碱化。', []), ('金门县政府在金门的教育上投入了数百万，平均每个学生两万元。福建的学校也接受越来越多父母在福建经商的台湾学生。县政府一直在努力鼓励台湾和大陆的大学在金门设立分校，并吸引大陆大陆学生到金门学习。', []), ('民国三十四年（1945年）第二次世界大战结束后，中华民国收复金门，设二镇四乡；民国35年（1946年）变更为二镇二乡。', [('民国三十四年（1945年）第二次世界大战结束后，中华民国收回金门，设二镇四乡；民国三十五年（1946年）变更为二镇二乡。', 0.8484848484848485)]), ('元朝统治（1343年-1368年），金门（旧称浯洲）金沙湾周围设有官镇埕、永安埕、田墩埕、浦头埕、沙美埕、斗门埕及南埕（今之刘澳、浦边至琼林一带）、保林（今之西浦头至古宁头一带）、东沙、烈屿（今之小金门上库至上林一带）等10个盐埕，官府为求实质统治与兴办教育，遂于浯洲凤翔里十七都后学村（今沙美），设置浯洲盐场司官阶从七品「今金沙国中游泳池至东埔一带」与浯洲书院「今沙美菜市场，渭阳 马阙 司令创建」，当时浯洲（金门旧称）盐产业到达颠峰，因盐场多集中于沙美区，造就沙美老街（砂尾街）万商云集与百业繁荣之极盛景况。', [('元朝统治时期（1343年-1368年），中央为求实质统治，遂于浯洲凤翔里十七都后学村（今沙美），设置浯洲盐场司（官职从七品官，在今金沙国中至东埔及荣光新村一带）及浯洲书院（现今之沙美菜市场），沙美因处金沙湾与汶水溪及金沙溪交汇处。在元代，系为金门地区最高行政机关浯洲盐场司与浯洲书院之旧址（元朝浯洲盐场司马阙司令兴建）。过往的金沙地区更是金门地区居住人口与风狮爷最为稠密的地方（金门全岛共64尊风狮爷，金沙镇则高达39尊、沙美有3尊）。', 0.3934426229508197)])]\n"
     ]
    }
   ],
   "source": [
    "n_pairs = sum([len(pair) for pair in sent_pairs_w2b])\n",
    "avg_pairs = n_pairs / len(sent_pairs_w2b)\n",
    "print(n_pairs, avg_pairs)\n",
    "print(sent_pairs_w2b[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "limiting-metadata",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7002 2.0\n",
      "[('民国三十四年（1945年）第二次世界大战结束后，中华民国收回金门，设二镇四乡；民国三十五年（1946年）变更为二镇二乡。', [('民国三十四年（1945年）第二次世界大战结束后，中华民国收复金门，设二镇四乡；民国35年（1946年）变更为二镇二乡。', 0.8484848484848485)]), ('御赐里琼林坊：琼林旧称平林，因贤才尽出故明熹宗天启五年被御赐为琼林。琼林村的蔡氏家族，是明朝中叶从河南开封迁徙而来，明熹宗天启五年，因为平林籍的进士蔡献臣赶走蛮夷有功，于是赐里名“琼林”。琼林家庙经文建会评其为“十四世宗祠”，为台湾地区“国家”二级古迹。', []), ('据史料记载：“鲁王为明太祖九世孙，名朱以海．京师既陷，转徙台州，张国雄等迎居绍兴，称鲁监国，督师江上，画钱塘而守。后为清兵所克，遁入海，依郑成功，辗转到金门，去监国号。成功初以礼待之，后渐懈，以海不能平，将往南澳，成功使人沉之海”；另据《辞海大事记》记载：康熙元年（1662年），鲁王薨于台湾，两处记载互相矛盾，还有待进一步考证。', []), ('2004年，金门县财政总收入1812637万元新台币；人均所得308202元新台币；人均生产总值329656元新台币。', []), ('金门属于亚热带季风气候，全年降雨多集中于四至八月，台风多生于七、八月，全年风向东风占8个月，每年五至八月为东南风及南风。因金门为在海峡中之岛屿，四面无高山屏障，中间则丘陵起伏，故风力较强，夏有西南海风的吹拂，每到清明时候常带来浓雾，台金交通常受影响；东有强烈的东北季风。', []), ('金门的地层，以花岗片麻岩为主，分布甚为广阔，约占总面积一半。岛上土壤概以砂土及裸露红壤土为代表。前者沙层厚、保水保肥力均差；后者表土薄、酸性重，腐植质少，皆不宜耕作，故岛上农作仅宜价值较低之耐旱性杂粮：如高粱、玉米、花生、番薯等。由于四面环海，浅滩深澳，鱼虾贝介类滋生，滨海居民乃讨生计于大海中，然因渔业资源有限，兼且幅员狭窄，地力贫瘠，雨量稀少，农产不丰，只有少量之杂粮与蔬菜。居民乃远渡重洋，谋生异域，或移居台澎，或远适南洋，金门华侨足迹遍布东南亚，人口总数达二十余万之众，自古就有“侨乡”之称。', []), ('由于发掘了鲁王真圹，在出土志里，说明旧时的谬误之处。据载王世系事迹綦详，卒年为王寅康熙元年（1662年），并云王素患哮疾，中痰而死。文武百官遂将其葬于金门东门外的青山上，前有湖水，右有石峰。王屡游其地，曾题“汉影云根”四字于石上。', []), ('明末清军入关后，明思宗自缢，天下大乱，南明各王四处流亡。鲁王朱以海辗转到金门，寓居七年之久，因时局太乱，死后秘密葬在金门，不敢立碑。1959年夏季，金门“国军”在旧金城东古岗湖西发现鲁王真冢。', []), ('金门的主要地形为起伏的花岗岩地形，岛上最高峰为太武山（其高为253米，其上有“毋忘在莒”蒋中正的手书），雄势雄伟，独冠屿上，近观之若兜鍪状，故名太武。其石纷纠萦纡若印章篆刻，又称“海印”，隔海远望，如仙人偃卧（仙人倒地卧不起之形），海上人因别呼之为“仙山”。', []), ('按“一个中国”原则和“九二共识”，其单方面表述为“中华民国福建省”。', [])]\n"
     ]
    }
   ],
   "source": [
    "n_pairs = sum([len(pair) for pair in sent_pairs_b2w])\n",
    "avg_pairs = n_pairs / len(sent_pairs_b2w)\n",
    "print(n_pairs, avg_pairs)\n",
    "print(sent_pairs_b2w[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "stuffed-girlfriend",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(pairs, maxlen=254, n_test=300, seed=\"ETST\"):\n",
    "    # remove long sentences\n",
    "    _pairs = pairs\n",
    "    pairs = []\n",
    "    for src, targs in _pairs:\n",
    "        if len(src) <= maxlen:\n",
    "            targs = [(targ, score) for targ, score in targs if len(targ) <= maxlen]\n",
    "            if targs:\n",
    "                pairs.append((src, targs))\n",
    "    \n",
    "    random.seed(seed)\n",
    "    random.shuffle(pairs)\n",
    "    random.seed()\n",
    "            \n",
    "    sep1 = -2 * n_test\n",
    "    sep2 = -n_test\n",
    "    return pairs[:sep1], pairs[sep1:sep2], pairs[sep2:]\n",
    "\n",
    "def pairs2datasets(pairs):\n",
    "    train_pairs, valid_pairs, test_pairs = split_data(pairs)\n",
    "    \n",
    "    train_ds = []\n",
    "    for src, targs in train_pairs:\n",
    "        train_ds += [(src, targ) for targ, score in targs]\n",
    "    \n",
    "    valid_ds = [(src, targs[0][0]) for src, targs in valid_pairs if targs]\n",
    "    test_ds = [(src, targs[0][0]) for src, targs in test_pairs if targs]\n",
    "    return train_ds, valid_ds, test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "coupled-afternoon",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"/home/zchen/encyclopedia-text-style-transfer/data/ETST/sents_pairs_b2w.json\"\n",
    "sent_pairs_b2w = load_json(file)\n",
    "file = \"/home/zchen/encyclopedia-text-style-transfer/data/ETST/sents_pairs_w2b.json\"\n",
    "sent_pairs_w2b = load_json(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "several-dialogue",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "815\n",
      "[('中华人民共和国主席、副主席每届任期同全国人民代表大会每届任期相同。', '《中华人民共和国宪法》第六十七条规定，宪法的解释权属于全国人民代表大会常务委员会。'), ('中华人民共和国主席、副主席每届任期同全国人民代表大会每届任期相同。', '《中华人民共和国宪法 (1954年)》，简称五四宪法，是中华人民共和国的第一部宪法，1954年9月20日经第一届全国人民代表大会第一次会议审议通过，因其在1954年颁布，故称其为“五四宪法”。中华人民共和国现行的八二宪法（第四部宪法）就是建立在五四宪法的基础之上。'), ('中华人民共和国主席、副主席每届任期同全国人民代表大会每届任期相同。', '1978年《中华人民共和国宪法》，通称七八宪法或1978宪法，是1978年3月5日在第五届全国人民代表大会第一次会议上通过的《中华人民共和国宪法》。'), ('中华人民共和国主席、副主席每届任期同全国人民代表大会每届任期相同。', '1954年9月20日，第一届全国人民代表大会第一次会议上，代表们共投票1197张，同意票为1197张，全票通过《中华人民共和国宪法》。'), ('中华人民共和国主席、副主席每届任期同全国人民代表大会每届任期相同。', '《中华人民共和国宪法》是中华人民共和国的根本法，拥有最高的法律效力。'), ('中华人民共和国主席、副主席每届任期同全国人民代表大会每届任期相同。', '经历2年多的修订，1982年12月4日，第五届全国人民代表大会第五次会议审议通过新的宪法，成为中华人民共和国的第四部宪法。该宪法建立在五四宪法的基础上，构成了现今宪法的主体部分，其主要的修订内容包括：'), ('中华人民共和国主席、副主席每届任期同全国人民代表大会每届任期相同。', '新宪法于1978年3月5日在第五届全国人民代表大会第一次会议上通过。'), ('中华人民共和国主席、副主席每届任期同全国人民代表大会每届任期相同。', '经历2年多的修订，1982年12月4日，第五届全国人民代表大会第五次会议审议通过新的宪法，成为中华人民共和国的第四部宪法（简称“八二宪法”）。该宪法建立在五四宪法的基础上，构成了现今宪法的主体部分，其主要的修订内容包括：'), ('中华人民共和国主席、副主席每届任期同全国人民代表大会每届任期相同。', '《中华人民共和国宪法 (1982年)》，简称八二宪法，是中华人民共和国的第四部宪法以及现行宪法，由1980年8月18日时任领导人邓小平提出全面修宪建议，历经2年多的修订，在1982年12月4日举行的第五届全国人民代表大会第五次会议中被表决通过。2018年的第五次修订版是该宪法的最新修正案。'), ('中华人民共和国主席、副主席每届任期同全国人民代表大会每届任期相同。', '1975年《中华人民共和国宪法》，是指1975年1月17日第四届全国人民代表大会第一次会议通过的《中华人民共和国宪法》（因其在1975年颁布，故称其为“七五宪法”或“1975宪法”）。这是中华人民共和国的第二部宪法，除序言外分总纲、国家机构、公民的基本权利和义务、国旗、国徽、首都，共四章三十条。')]\n",
      "300\n",
      "[('依照前款规定采取非和平方式及其他必要措施，由国务院、中央军事委员会决定和组织实施，并及时向全国人民代表大会常务委员会报告。', '外界将焦点集中在三大条件中的最后一项，即“和平统一的可能性完全丧失”，这被认为是一项可以被非常灵活解释的条件。另外第八条也允许国务院在必要时先采取行动，随后再向全国最高权力机关人民代表大会通报，等于授权国务院、中央军事委员会可以先斩后奏。第九条则要求在“采取非和平方式及其他必要措施”时，国家尽最大可能「保护台湾平民和在台湾的外国人的生命财产安全和其他正当权益，减少损失；同时，国家依法保护台湾同胞在中国其他地区的权利和利益」。'), ('1971年10月25日，联合国大会第1976次会议以76票赞成、35票反对、17票弃权的压倒多数，通过了阿尔巴尼亚、阿尔及利亚等23个国家提出的要求“恢复中华人民共和国在联合国的一切合法权利，立即把蒋介石集团的代表从联合国一切机构中驱逐出去”的提案。26日，中国代理外交部长姬鹏飞收到联合国秘书长吴丹发来的正式通知，中华人民共和国在联合国和安理会中被非法剥夺了20多年的席位得到恢复。', '大会以76票赞成、35票反对、17票弃权的结果，通过阿尔巴尼亚、阿尔及利亚、罗马尼亚等23个国家联合提出的关于“恢复中华人民共和国在联合国组织中的合法权利问题”的A/L.630决议草案及1、2号附加文件。根据联合国宪章和联合国大会议事规则，这项提案通过以后立即成为联合国大会的正式决议。'), ('1954年6月，台当局在其控制的莆田县乌丘屿设置乌丘乡（乌丘村），由于莆田县主体已全部由中国大陆统治，故指定暂由金门县代管，此时金门县辖金城镇、金沙镇、金宁乡、金湖乡、金山乡、烈屿乡、乌丘乡（乌丘村）等二镇五乡。', '金门县辖有三镇（金城镇、金湖镇、金沙镇）、三乡（金宁乡、烈屿乡、乌坵乡（代管））。'), ('“一纲四目”的提出，还促使蒋介石产生了与中国共产党第三次合作的念头。', '一纲四目，是1963年中华人民共和国国务院总理周恩来归纳的中国共产党对台政策。'), ('1954年9月，第一届全国人民代表大会举行第一次会议，通过了《中华人民共和国宪法》。同年12月，召开了政协第二届全国委员会第一次会议，制定了《中国人民政治协商会议章程》。《章程》宣告，共同纲领已经为宪法所代替，人民政协全体会议代行全国人民代表大会职权的任务已经完成。但是人民政协作为人民民主统一战线组织，将继续存在和发挥作用。', '1954年，由于宪法所规定的全国人民代表大会和地方各级人民代表大会的建立，同年召开了第一届全国人民代表大会第一次会议，中国人民政治协商会议的立法职能让渡予全国人大，自此，政协便成为人民民主统一战线组织。'), ('截至2005年，连江县境内有“三湾”（罗源湾、黄岐湾、定海湾）、“三口”（可门口、闽江口、敖江口），七条通道（温福铁路、沈海高速、沈海高速复线、104国道、201省道、福州东南绕城高速、福州西绕城高速）贯穿全境，已开通城关、贵安、琯头、丹阳、马鼻、浦口六个高速互通口，并设有连江火车站、透堡铁路货物中转站。', '连江县目前有温福铁路过境，连江火车站位于江南乡花坞村。 截至2017年2月，每日经停连江站全部列车共有25趟，无始发与终到列车。连江境内已通车的高速公路有（途径丹阳、洋门、连江、琯头）和（途径洋门、贵安），另有、过境。'), ('最高人民检察院检察长每届任期同全国人民代表大会每届任期相同，连续任职不得超过两届。', '新宪法于1978年3月5日在第五届全国人民代表大会第一次会议上通过。'), ('2009年4月25日－26日，两会领导人第三次会谈在南京举行。', '两会协商谈判.'), ('香港特别行政区行政长官是特别行政区的首长，代表香港特别行政区，既对中央人民政府负责又对香港特别行政区负责。行政长官还是特别行政区政府的首长，依法履行基本法授予的领导特别行政区政府、负责执行基本法以及其他各项职权。行政长官在行使职权时须执行中央人民政府就香港基本法有关事项发出的指令。', '香港特别行政区行政长官是香港特别行政区政府首长（简称特首），代表香港特别行政区政府。行政长官每届任期为5年（由选举年7月1日起），可连任1次。行政长官领导维护国家安全委员会，担任主席，向中央人民政府（国务院）负责。由行政长官主持的行政会议为政府最高行政决策机关。现时香港行政长官由分别来自三十八个界别分组，代表不同行业、专业、劳工、社会服务团体及区域组织的1200人组成的选举委员会选出，并由国务院总理签署国务院令任命。'), ('日据时期，在大陆“五四”新文学运动的影响和指导下产生和发展起了台湾新文学运动，', '皇民化运动时期（1937～1945）.')]\n",
      "300\n",
      "[('民进党的党章虽然开明宗义地表明，民进党是一个追求“民主”和“进步”的政党，然而民进党的党纲则素有“台独党纲”之称。其实，民进党的党纲经过多次修订，不断地朝“缓和”方向变化，但目前仍不脱“台独”色彩。近年来，民进党又通过一系列决议文，来掩饰、修正党纲中赤裸裸的“台独”主张。除了“台独”内容外，民进党的党纲还主张建立“民主自由的法政秩序”、推行成长均衡的经济财政政策、建立“公平开放的福利社会”、培养“创新进步的教育文化”，以及推行“和平独立”的“国防外交”政策等', '民进党主张遵循民主与自由的宪政秩序；施行成长均衡的经济财政；建立公平开放的福利社会；创新精进的教育文化；全球国际的国防外交；以台湾的名义来争取加入联合国；反对中华人民共和国用文攻武吓胁迫台湾。'), ('台湾是中国领土不可分割的一部分。', '台湾与中国大陆的用字差异.'), ('中国人民政治协商会议全国委员会港澳台侨委员会', '根据《》的规定，中国人民政治协商会议全国委员会全体会议的职权为：'), ('五、协助保障台湾地区人民在大陆地区停留期间之合法权益。', '台湾地区人民在大陆地区停留权益保障及两岸人民往来与人身安全之协调、联系及处理。'), ('连江县（马祖）位于台湾海峡西北方的马祖列岛，隶属于福建省福州市连江县，包含南竿、北竿、高登、亮岛、大丘、小丘、东莒、西莒、东引、西引等十个岛屿，群岛罗列，面临闽江口、连江口及罗源湾，与大陆只有一水之隔，为海运要冲，一直是扮演前哨的角色', '连江县（闽东语平话字：）中华民国福建省的县，位于台湾海峡正北方，面临闽江口、敖江口和罗源湾，与中国大陆只有一水之隔，为海运要冲。现主要由南竿岛、北竿岛、高登岛、亮岛、东莒岛、西莒岛、东引岛、西引岛及其附属小岛共计三十六个岛屿、礁屿组成。'), ('台北市是台湾地区的最高行政管理机构所在地，亦是台湾政治、经济与文化发展的中心。', '台湾可以指地理范围、文化领域或行政区划。'), ('在此，中国政府认为有必要就以下几个问题重申自己的立场和政策。', '中国大陆方面的立场.'), ('1895年4月17日上午11时40分，李鸿章代表清政府与日本在马关春帆楼签订丧权辱国的《马关条约》，其主要内容包括：中国承认朝鲜独立；割让台湾岛及其附属岛屿、澎湖列岛与辽东半岛给日本；赔偿日本2亿两白银；开放沙市、重庆、苏州、杭州为通商口岸；允许日本人在通商口岸开设工厂。1895年5月8日，中日两国在芝罘（今山东烟台）交换两国皇帝的批准书，条约正式生效。（图册中马关春帆楼图来源', '4月17日议和谈判期间，双方经历数次会面谈判及书面协调后，日方降低要求，将赔款缩减为库平银二亿两白银；缩减辽东半岛割让范围（由北纬41度线改为鸭绿江口-安平河口-凤城-海城-营口-辽河口一线）；减去顺天、湘潭、梧州三处通商口岸……等等。于1895年（明治28年）4月17日，光绪二十一年三月二十三日，日本当地时间上午11时40分，中日两国全权代表在春帆楼二楼签署了《日清讲和条约》，即《马关条约》。'), ('1971年9月29日，22个成员国向联合国提交了另一项决议草案A/L.632以及附加文件1、2号。该决议草案提议：任何试图剥夺“中华民国”代表权的提案都是涉及联合国宪章18条的重大问题，因此需要三分之二的会员国赞成方可通过。 A/L.632草案及附加文件1、2号在1971年10月25日的表决过程中以59票反对，55票赞成，15票弃权未获通过。', '1971年9月29日，澳大利亚、日本等22个成员国向联合国提交了「重要问题」的决议草案A/L.632以及附加文件1、2号。该决议草案提议：任何试图剥夺中华民国代表权的提案都是涉及的重大问题，因此应以到会及投票之会员国三分之二多数决定之。因其较两阿提案晚提出，序号靠后，一般情况下应按序号顺序表决，所以澳、日等国又提出此草案需于两阿提案之前表决的动议，后于25日先对此提前表决动议表决并获通过。'), ('事实表明，要进行两岸商谈，首先要确立坚持一个中国原则的共识，没有这样一个共识作为基础，两岸事务性商谈无法进行。因此，在两会1992年3月第一次工作性商谈结束后，针对台湾当局的理解，并考虑到部分台湾民众的疑虑，海协常务副会长唐树备就两岸事务性商谈中坚持一个中国原则的态度问题，作了进一步阐述。', '1997年5月14日，海协会副会长唐树备曾在接受采访时，认为台湾方面将香港会谈的共识归结为「一个中国，各自表述」并不正确，认为共识是「海峡两岸均坚持一个中国之原则」：')]\n"
     ]
    }
   ],
   "source": [
    "train_ds, valid_ds, test_ds = pairs2datasets(sent_pairs_b2w)\n",
    "print(len(train_ds))\n",
    "print(train_ds[:10])\n",
    "print(len(valid_ds))\n",
    "print(valid_ds[:10])\n",
    "print(len(test_ds))\n",
    "print(test_ds[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ethical-allergy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataset(dataset, src_file, targ_file):\n",
    "    src, targ = zip(*dataset)\n",
    "    save_corpus(src, src_file)\n",
    "    save_corpus(targ, targ_file)\n",
    "    \n",
    "def save_corpus(sent_list, file):\n",
    "    corpus = '\\n'.join(sent_list) + '\\n'\n",
    "    with open(file, 'w', encoding=\"utf-8\") as f:\n",
    "        f.write(corpus)\n",
    "    \n",
    "for dataset, split in [(train_ds, \"train\"), (valid_ds, \"valid\"), (test_ds, \"test\")]:\n",
    "    src_file = f\"/home/zchen/XLM_ETST/data/baidu-wiki/txt/baidu-wiki.baidu.{split}_raw\"\n",
    "    targ_file = f\"/home/zchen/XLM_ETST/data/baidu-wiki/txt/baidu-wiki.wiki.{split}_raw\"\n",
    "    save_dataset(dataset, src_file, targ_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "psychological-marathon",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sentence_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2468837/2073699464.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"'{sentence}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sentence_list' is not defined"
     ]
    }
   ],
   "source": [
    "def print_word_pos_sentence(word_sentence, pos_sentence):\n",
    "    assert len(word_sentence) == len(pos_sentence)\n",
    "    for word, pos in zip(word_sentence, pos_sentence):\n",
    "        print(f\"{word}({pos})\", end=\"\\u3000\")\n",
    "    print()\n",
    "    return\n",
    "    \n",
    "for i, sentence in enumerate(sentence_list):\n",
    "    print()\n",
    "    print(f\"'{sentence}'\")\n",
    "    print_word_pos_sentence(word_sentence_list[i],  pos_sentence_list[i])\n",
    "    for entity in sorted(entity_sentence_list[i]):\n",
    "        print(entity)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text_style_transfer",
   "language": "python",
   "name": "text_style_transfer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
